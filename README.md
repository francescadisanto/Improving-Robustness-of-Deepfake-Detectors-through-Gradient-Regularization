# Improving-Robustness-of-Deepfake-Detectors-through-Gradient-Regularization

# Overview

This project addresses the challenge of building robust deepfake detectors by applying gradient regularization to improve both generalization and resistance to adversarial attacks. Our method is based on **EfficientNet-B0** and introduces a **gradient regularization technique** to improve model generalization and stability. 
To thoroughly evaluate robustness, we subject the models to **three types of adversarial attacks**:
- **FGSM (Fast Gradient Sign Method)**  
- **PGD (Projected Gradient Descent)**
- **BIM (Basic Iterative Method)**

## Dataset

We use the **DFFD (DeepFake Face Dataset)**, a collection of real and manipulated face images from various generation methods. It includes:

- **Real images** : from the *FFHQ* dataset
- **Fake images** generated using:
  - **FaceApp**
  - **StyleGAN** (CelebA & FFHQ variants)
  - **StarGAN**
  - **PG-GAN** (versions v1 and v2)

### Dataset Split
The dataset is organized into three distinct subsets:
- **Training Set**: a mixture of real and fake samples. We apply **class-weighted loss** to compensate for label imbalance (real: ~10k, fake: ~56k).
- **Validation Set**: used to monitor generalization and fine-tune hyperparameters.
- **Test Set**: used for final performance and robustness evaluation.

### Data Augmentation

To improve generalization and reduce overfitting, the training set undergoes a series of data augmentation transformations. These include:

- Random resized cropping (zoom-in/out)
- Random horizontal flipping
- Random color jitter (brightness, contrast, saturation, hue)
- Random rotation
- Tensor conversion and normalization

Meanwhile, validation and test sets are only resized and normalized to ensure evaluation consistency.

```python
# ImageNet statistics for EfficientNet-B0
efficientnet_mean = [0.485, 0.456, 0.406]
efficientnet_std = [0.229, 0.224, 0.225]

# Training augmentation
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean=efficientnet_mean, std=efficientnet_std)
])

# Validation / Test preprocessing
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=efficientnet_mean, std=efficientnet_std)
])
```

# Key Features

- **Pretrained EfficientNet-B0** : used as the backbone for both baseline and regularized models, initialized with ImageNet weights.
- **Gradient Regularization** :improves generalization and robustness by injecting noise into shallow feature statistics (mean and variance) during training.
- **Adversarial Training** to enhance robustness
- **Robustness Against Adversarial Attacks** : evaluation and visualization under three adversarial attack methods:
  - **FGSM** (Fast Gradient Sign Method)
  - **PGD** (Projected Gradient Descent)
  - **BIM** (Basic Iterative Method)

- **Performance Metrics** : evaluation using:
  - **Accuracy**
  - **Macro F1 Score**
  - **AUC (Area Under Curve)**
  - **IINC** (Interpretability-Informed Consistency)
    
- **CAM (Class Activation Map) visualizations** for clean and adversarial examples.
- **Comparison Plots**  

# Architecture & Models

This project implements and compares two models:
- A **Baseline** model trained with standard loss
- A **Regularized** model trained with gradient regularization


## Baseline model

The baseline is a **binary classifier** built on a pretrained **EfficientNet-B0**. The final classification head is replaced with a fully connected layer with two outputs for binary classification (real vs fake).

It is trained using **class-weighted cross-entropy loss** to handle the class imbalance between real and fake samples.
The goal is to accurately distinguish real faces from fake ones generated by various GAN-based methods.

## Gradient-regularized Model

To enhance generalization and improve robustness against adversarial attacks, we implemented a **gradient regularization strategy** inspired by Guan et al. (2024).

This method specifically addresses a known limitation of deepfake detectors: their tendency to overfit to superficial texture cues present in the training data. These texture patterns, often specific to certain manipulation techniques, lead to poor generalization on unseen fake sources.

Instead of regularizing the model with respect to its weights, our approach **regularizes the model’s response to perturbations in the shallow feature statistics** — specifically, the **mean (µ)** and **standard deviation (σ)** extracted from early convolutional layers.

- Two forward passes are computed:
  - `forward_clean(x)` uses original shallow features.
  - `forward_perturbed(x)` perturbs the shallow features before continuing through the deeper layers.

This allows the model to learn representations that are **less dependent on texture-specific cues** and more robust to subtle variations in the input.

We avoid computing gradients ∂L/∂µ explicitly for stability and efficiency, and instead introduce **random noise to the feature statistics** at training time to simulate gradient regularization.

 The total loss combines both clean and perturbed outputs:
```python
loss_total = (1 - α) * loss_clean + α * loss_perturbed

```

Where:
- `alpha` balances the contribution of regularization (e.g., 0.5)  
- `epsilon` controls the strength of the perturbation (e.g., 0.05)  
- Both components use standard **cross-entropy loss**

##  Adversarial Evaluation

Robustness is tested using three adversarial attacks on both the baseline and the regularized model:

- **FGSM**  
- **PGD**  
- **BIM**

Performance is measured across different epsilon values (perturbation strength), and comparisons are made in terms of accuracy, F1 score, and IINC to assess model stability under attack.

## Visual Analysis

- **Grad-CAM** is used to visualize class activation maps under clean and adversarial settings  
- **IINC** metric quantifies the change in CAM between clean and perturbed inputs  
- Helps assess the model’s interpretability and resilience

---

## Results Summary

- Regularized models show **higher robustness** across FGSM, PGD, and BIM attacks  
- **IINC** is consistently lower in the regularized model, showing better feature invariance  
- Accuracy and F1 remain more stable as epsilon increases


# References

1. W. Guan, W. Wang, J. Dong and B. Peng, (2024). Improving Generalization of Deepfake Detectors by
Imposing Gradient Regularization, In IEEE Transactions on Information Forensics and Security, vol. 19, pp.
5345-5356.
2. M. Tan and Q. Le, (2019). EfficientNet: Rethinking model scaling for convolutional neural networks. In
Proc. Int. Conf. Mach. Learn., pp. 6105–6114.
3. On the Detection of Digital Face Manipulation Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, Anil
Jain, (2020), In: Proceedings of IEEE Computer Vision and Pattern Recognition (CVPR 2020), Seattle,
WA, Jun. 2020
4. Abbasi, M., V´az, P., Silva, J. and Martins, P. (2025). Comprehensive Evaluation of Deepfake Detection
Models: Accuracy, Generalization, and Resilience to Adversarial Attacks. Applied Sciences, 15(3), 1225.




